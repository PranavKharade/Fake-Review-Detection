{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1dcb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "--- Loading and Reprocessing Data ---\n",
      "\n",
      "--- Data Ready ---\n",
      "\n",
      "--- Starting Improvement 1: Re-training with 2 Epochs ---\n",
      "WARNING:tensorflow:From d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created and compiled successfully.\n",
      "\n",
      "Starting training for 2 epochs...\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:From d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "54/54 [==============================] - 465s 8s/step - loss: 0.5100 - accuracy: 0.7419 - val_loss: 0.3483 - val_accuracy: 0.8125\n",
      "Epoch 2/2\n",
      "54/54 [==============================] - 1124s 21s/step - loss: 0.2844 - accuracy: 0.8935 - val_loss: 0.4816 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fake_Review_Detector\\venv\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Improvement Step 1 Complete! ---\n",
      "New model saved as 'fake_review_model_v2.keras'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tf_keras\n",
    "\n",
    "# --- Phase 1 & 2 Recap: Load and Preprocess Data ---\n",
    "# This ensures this notebook can run independently.\n",
    "print(\"--- Loading and Reprocessing Data ---\\n\")\n",
    "# --- IMPORTANT: Update this path if needed! ---\n",
    "# This is the corrected path\n",
    "base_path = 'D:/Fake_Review_Detector/op_spam_v1.4/op_spam_v1.4'\n",
    "# Load data\n",
    "reviews = []\n",
    "labels = []\n",
    "for label_type in ['deceptive_from_MTurk', 'truthful_from_TripAdvisor']:\n",
    "    for polarity in ['positive_polarity', 'negative_polarity']:\n",
    "        path = os.path.join(base_path, polarity, label_type)\n",
    "        files = glob.glob(os.path.join(path, 'fold*', '*.txt'))\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reviews.append(f.read())\n",
    "                labels.append(1 if 'deceptive' in label_type else 0)\n",
    "df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Clean and Tokenize the data...\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "tokenized_data = tokenizer(\n",
    "    df['cleaned_review'].tolist(), padding='max_length', truncation=True, return_tensors='np', max_length=256\n",
    ")\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']\n",
    "labels = np.array(df['label'].values)\n",
    "\n",
    "# Create train and test splits\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    input_ids, attention_mask, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"--- Data Ready ---\\n\")\n",
    "\n",
    "\n",
    "# --- PHASE 3 (REVISED): Re-build and Re-train the Model ---\n",
    "print(\"--- Starting Improvement 1: Re-training with 2 Epochs ---\")\n",
    "\n",
    "def create_model():\n",
    "    # Define the two input layers for our tokenized data\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask_layer = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # ALBERT layer from Hugging Face\n",
    "    albert_model = TFAlbertModel.from_pretrained('albert-base-v2', from_pt=True)\n",
    "    albert_outputs = albert_model(input_ids_layer, attention_mask=attention_mask_layer)\n",
    "    \n",
    "    sequence_output = albert_outputs.last_hidden_state\n",
    "\n",
    "    # A Bi-directional LSTM layer to understand the sequence of words\n",
    "    lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(sequence_output)\n",
    "\n",
    "    # A final dense layer for classification\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model created and compiled successfully.\\n\")\n",
    "print(\"Starting training for 2 epochs...\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_ids, X_train_mask],\n",
    "    y_train,\n",
    "    epochs=2, # *** THE ONLY CHANGE IS HERE: 2 epochs instead of 3 ***\n",
    "    batch_size=16,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Save the new, improved model with a different name\n",
    "model_save_path = 'fake_review_model_v2.keras'\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"\\n--- Improvement Step 1 Complete! ---\")\n",
    "print(f\"New model saved as '{model_save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
